{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Comparisons\n",
    "\n",
    "> Can't rely solely on OpenAI and Azure for WiseGuyAI.com. Need to compare the other LLMs based on cost, performance, and quality.\n",
    "\n",
    "## LLMs\n",
    "\n",
    "1. OpenAI\n",
    "2. Cohere\n",
    "3. A121\n",
    "4. Huggingface Hub\n",
    "5. Azure OpenAI\n",
    "6. Manifest\n",
    "7. Goose Al\n",
    "8. Cerebrium\n",
    "9. Petals\n",
    "10. Forefront AI\n",
    "11. PromptLayer OpenAI\n",
    "12. Anthropic\n",
    "13. Self-Hosted Models (via Runhouse)\n",
    "\n",
    "All LLMs will need to be tested to with the same prompt to see how they compare. We need to keep track of the duration of each request. We also need to try each request at least 3 times to be sure that it gets it correct. \n",
    "\n",
    "We can figure out the prices before we even run most of these models. \n",
    "\n",
    "Gonna use langchain to hopefully make this a little bit easier.\n",
    "\n",
    "Need to sign up for all these services -_-\n",
    "\n",
    "Will track the progress of each by hand in this document + notebooks whenever possible. Honestly maybe this should be a notebook just keep better track.\n",
    "\n",
    "### Define\n",
    "\n",
    "- **Cost**: How much it will cost to run a lot of questions through this model.\n",
    "- **Performance**: How fast it will take to get a response from the model. Latency isn't that important but a slow response is a bad sign that outages could happen.\n",
    "- **Quality**: How good the responses are. This is the most important metric. We need to make sure that the responses are good enough to be used in production."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'langchain[llms]' --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API3 Prompt loaded. Character count: 4292\n",
      "Woodwork Prompt loaded. Character count: 4160\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import *\n",
    "import json\n",
    "from IPython.display import Markdown\n",
    "import time\n",
    "\n",
    "\n",
    "# open prompt.txt\n",
    "with open('prompt.txt', 'r') as f:\n",
    "    api3_prompt = f.read()\n",
    "\n",
    "print(f'API3 Prompt loaded. Character count: {len(api3_prompt)}')\n",
    "\n",
    "with open('woodworkprompt.txt', 'r') as f:\n",
    "    wood_prompt = f.read()\n",
    "print(f'Woodwork Prompt loaded. Character count: {len(wood_prompt)}')\n",
    "\n",
    "# Open keys.json. If it doesnt exist, copy it from example.keys.json\n",
    "try:\n",
    "    with open('keys.json', 'r') as f:\n",
    "        keys = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    print('Keys not found. Add your API keys to keys.json')\n",
    "    with open('example.keys.json', 'r') as f:\n",
    "        keys = json.load(f)\n",
    "    with open('keys.json', 'w') as f:\n",
    "        json.dump(keys, f)\n",
    "\n",
    "\n",
    "def ask(llm, price, prompt):\n",
    "    start_time = time.time()\n",
    "    results = llm.generate([prompt], stop=['Q: '])\n",
    "    duration = time.time() - start_time\n",
    "    first_generation = results.generations[0][0].text\n",
    "    # if results.llm_output.token_usage exists, include that in the response, otherwise leave it undefined\n",
    "    try:\n",
    "        token_usage = results.llm_output['token_usage']\n",
    "        cost = token_usage['total_tokens'] * price\n",
    "        cost = round(cost, 3)\n",
    "    except:\n",
    "        token_usage = None\n",
    "        cost = None\n",
    "\n",
    "    display(Markdown(f\"\"\"### Answer:\\n\\n----------------\\n\\n\n",
    "{first_generation}\n",
    "\\n\\n----------------\\n\\n\"\"\"))\n",
    "    print(f'Duration: {duration} seconds')\n",
    "\n",
    "    if token_usage:\n",
    "        print(f'Token usage: {token_usage}')\n",
    "        print(f'Cost: ${cost} or ${cost * 1000} per 1000 Questions')\n",
    "\n",
    "    return {'answer': first_generation, 'token_usage': token_usage, 'cost': cost, 'duration': duration}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This one will be our control. OpenAI is SOTA but they have insane downtime and are the most expensive. \n",
    "\n",
    "#### Pricing\n",
    "\n",
    "- $0.02 per 1,000 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMResult(generations=[[Generation(text=' You can use the `deriveAirnodeXpub` function in the [@airnode/airnode-admin](https://github.com/api3dao/airnode/blob/master/packages/airnode-admin/src/implementation.ts) package in Node.js like this:\\n\\n```js\\nconst airnodeXpub = deriveAirnodeXpub(airnodeMnemonic);\\n```\\n\\nThis will derive the Airnode xpub from the Airnode mnemonic.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'total_tokens': 1555, 'completion_tokens': 111, 'prompt_tokens': 1444}})\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Answer:\n",
       "\n",
       "----------------\n",
       "\n",
       "\n",
       " You can use the `deriveAirnodeXpub` function in the [@airnode/airnode-admin](https://github.com/api3dao/airnode/blob/master/packages/airnode-admin/src/implementation.ts) package in Node.js like this:\n",
       "\n",
       "```js\n",
       "const airnodeXpub = deriveAirnodeXpub(airnodeMnemonic);\n",
       "```\n",
       "\n",
       "This will derive the Airnode xpub from the Airnode mnemonic.\n",
       "\n",
       "\n",
       "----------------\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 4.660804986953735 seconds\n",
      "Token usage: {'total_tokens': 1555, 'completion_tokens': 111, 'prompt_tokens': 1444}\n",
      "Cost: $0.031 or $31.0 per 1000 Questions\n",
      "LLMResult(generations=[[Generation(text=' Wait till the caulk dries before applying the mold release.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'total_tokens': 1088, 'completion_tokens': 13, 'prompt_tokens': 1075}})\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Answer:\n",
       "\n",
       "----------------\n",
       "\n",
       "\n",
       " Wait till the caulk dries before applying the mold release.\n",
       "\n",
       "\n",
       "----------------\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 1.284977912902832 seconds\n",
      "Token usage: {'total_tokens': 1088, 'completion_tokens': 13, 'prompt_tokens': 1075}\n",
      "Cost: $0.022 or $22.0 per 1000 Questions\n"
     ]
    }
   ],
   "source": [
    "price = 0.02 / 1000  # 0.02 USD per 1000 tokens\n",
    "\n",
    "# If keys['openai'] falsy, throw error\n",
    "if not keys['openai']:\n",
    "    raise ValueError('OpenAI API key not found. Add your API key to keys.json')\n",
    "\n",
    "\n",
    "openai = OpenAI(model_name=\"text-davinci-003\",\n",
    "                openai_api_key=keys['openai'], temperature=0.6)\n",
    "\n",
    "openai_results = [ask(openai, price, api3_prompt),\n",
    "                  ask(openai, price, wood_prompt)]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "| Cost | Performance | Quality |\n",
    "|------|-------------|---------|\n",
    "| 1  |     2      |    5   |\n",
    "\n",
    "At $0.03 per question, this will probably be our most expensive model. Average response time was around 5 seconds. I know from experience they have a lot of downtime. The answer quality is amazing. Exactly what I am looking for. Truly SOTA.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohere AI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have tried this one. Its pretty good and cheaper than OpenAI but not by much. Gonna sign up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Answer:\n",
       "\n",
       "----------------\n",
       "\n",
       "\n",
       " You can use the `deriveAirnodeXpub` function in the [@airnode/airnode-admin](https://docs.api3.org/airnode/v0.10/) package in Node.js like this:\n",
       "\n",
       "```js\n",
       "const xpub = await deriveAirnodeXpub(airnodeXpub, airnodeAddress);\n",
       "```\n",
       "\n",
       "This will derive the Airnode xpub from the Airnode xpub, the Airnode address and the Airnode mnemonic.\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "----------------\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 22.349201917648315 seconds\n",
      "Cost: $0.012 or $12.0 per 1000 Questions\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Answer:\n",
       "\n",
       "----------------\n",
       "\n",
       "\n",
       " You can recoat immediately. And it won't dry out over weeks or longer.\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "----------------\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 10.559345006942749 seconds\n",
      "Cost: $0.011 or $11.0 per 1000 Questions\n"
     ]
    }
   ],
   "source": [
    "cohere = Cohere(cohere_api_key=keys['cohere'], temperature=0.1)\n",
    "# 2.5 USD per 1000 units of 1000 characters. Price per character\n",
    "price = (2.5 / 1000) / 1000\n",
    "\n",
    "\n",
    "def ask_cohere(prompt):\n",
    "    result = ask(cohere, price, prompt)\n",
    "    total_characters = len(result['answer']) + len(prompt)\n",
    "    cost = total_characters * price\n",
    "    cost = round(cost, 3)\n",
    "    print(f'Cost: ${cost} or ${cost * 1000} per 1000 Questions')\n",
    "    result['cost'] = cost\n",
    "    return result\n",
    "\n",
    "\n",
    "cohere_results = [ask_cohere(api3_prompt), ask_cohere(wood_prompt)]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "| Cost | Performance | Quality |\n",
    "|------|-------------|---------|\n",
    "| 4  |     2      |    3   |\n",
    "\n",
    "\n",
    "Much better price! Only around ~$0.011 per question! Completely fails at the coding challenge but nails the context challenge. Could be a useful cheaper alternative. Much slower though. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb2c640052a679058fa963edb19c73c9244639affc4049c356a2be62de116d7b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
