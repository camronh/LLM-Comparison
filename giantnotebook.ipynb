{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Comparisons\n",
    "\n",
    "> Can't rely solely on OpenAI and Azure for WiseGuyAI.com. Need to compare the other LLMs based on cost, performance, and quality.\n",
    "\n",
    "## LLMs\n",
    "\n",
    "1. OpenAI\n",
    "2. Cohere\n",
    "3. A121\n",
    "4. Huggingface Hub\n",
    "5. Azure OpenAI\n",
    "6. Manifest\n",
    "7. Goose Al\n",
    "8. Cerebrium\n",
    "9. Petals\n",
    "10. Forefront AI\n",
    "11. PromptLayer OpenAI\n",
    "12. Anthropic\n",
    "13. Self-Hosted Models (via Runhouse)\n",
    "\n",
    "All LLMs will need to be tested to with the same prompt to see how they compare. We need to keep track of the duration of each request. We also need to try each request at least 3 times to be sure that it gets it correct. \n",
    "\n",
    "We can figure out the prices before we even run most of these models. \n",
    "\n",
    "Gonna use langchain to hopefully make this a little bit easier.\n",
    "\n",
    "Need to sign up for all these services -_-\n",
    "\n",
    "Will track the progress of each by hand in this document + notebooks whenever possible. Honestly maybe this should be a notebook just keep better track.\n",
    "\n",
    "### Define\n",
    "\n",
    "- **Cost**: How much it will cost to run a lot of questions through this model.\n",
    "- **Performance**: How fast it will take to get a response from the model. Latency isn't that important but a slow response is a bad sign that outages could happen.\n",
    "- **Quality**: How good the responses are. This is the most important metric. We need to make sure that the responses are good enough to be used in production."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'langchain[llms]' --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API3 Prompt loaded. Character count: 4292\n",
      "Woodwork Prompt loaded. Character count: 4160\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import *\n",
    "import json\n",
    "from IPython.display import Markdown\n",
    "import time\n",
    "\n",
    "\n",
    "# open prompt.txt\n",
    "with open('prompt.txt', 'r') as f:\n",
    "    api3_prompt = f.read()\n",
    "\n",
    "print(f'API3 Prompt loaded. Character count: {len(api3_prompt)}')\n",
    "\n",
    "with open('woodworkprompt.txt', 'r') as f:\n",
    "    wood_prompt = f.read()\n",
    "print(f'Woodwork Prompt loaded. Character count: {len(wood_prompt)}')\n",
    "\n",
    "# Open keys.json. If it doesnt exist, copy it from example.keys.json\n",
    "try:\n",
    "    with open('keys.json', 'r') as f:\n",
    "        keys = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    print('Keys not found. Add your API keys to keys.json')\n",
    "    with open('example.keys.json', 'r') as f:\n",
    "        keys = json.load(f)\n",
    "    with open('keys.json', 'w') as f:\n",
    "        json.dump(keys, f)\n",
    "\n",
    "\n",
    "def ask(llm, prompt):\n",
    "    start_time = time.time()\n",
    "    results = llm.generate([prompt], stop=['Q: '])\n",
    "    duration = time.time() - start_time\n",
    "    first_generation = results.generations[0][0].text\n",
    "    # if results.llm_output.token_usage exists, include that in the response, otherwise leave it undefined\n",
    "\n",
    "    return {'answer': first_generation, \"llm_output\": results.llm_output, 'duration': duration}\n",
    "\n",
    "\n",
    "def render_answer(answer):\n",
    "    display(Markdown(f\"\"\"### Answer:\\n\\n----------------\\n\\n\n",
    "{answer}\n",
    "\\n\\n----------------\\n\\n\"\"\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This one will be our control. OpenAI is SOTA but they have insane downtime and are the most expensive. \n",
    "\n",
    "#### Pricing\n",
    "\n",
    "- $0.02 per 1,000 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Answer:\n",
       "\n",
       "----------------\n",
       "\n",
       "\n",
       " You can use the `deriveAirnodeXpub` function in the [@airnode/airnode-admin](https://github.com/api3dao/airnode/blob/master/packages/airnode-admin/src/implementation.ts#L135) package in Node.js like this:\n",
       "\n",
       "```js\n",
       "const xpub = deriveAirnodeXpub(airnodeMnemonic);\n",
       "```\n",
       "\n",
       "This will derive the xpub from the Airnode mnemonic.\n",
       "\n",
       "\n",
       "----------------\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 8.14532995223999 seconds\n",
      "Token usage: {'total_tokens': 1554, 'completion_tokens': 110, 'prompt_tokens': 1444}\n",
      "Cost: $0.031080000000000003 or $31.080000000000002 per 1000 Questions\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Answer:\n",
       "\n",
       "----------------\n",
       "\n",
       "\n",
       " Wait until the caulk dries before applying the mold release.\n",
       "\n",
       "\n",
       "----------------\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 1.6903250217437744 seconds\n",
      "Token usage: {'total_tokens': 1088, 'completion_tokens': 13, 'prompt_tokens': 1075}\n",
      "Cost: $0.02176 or $21.76 per 1000 Questions\n"
     ]
    }
   ],
   "source": [
    "price = 0.02 / 1000  # 0.02 USD per 1000 tokens\n",
    "\n",
    "if not keys['openai']:\n",
    "    raise ValueError('OpenAI API key not found. Add your API key to keys.json')\n",
    "\n",
    "\n",
    "openai = OpenAI(openai_api_key=keys['openai'], temperature=0.6)\n",
    "\n",
    "\n",
    "def ask_openai(prompt):\n",
    "    result = ask(openai, prompt)\n",
    "    render_answer(result['answer'])\n",
    "    print(f'Duration: {result[\"duration\"]} seconds')\n",
    "    token_usage = result['llm_output']['token_usage']\n",
    "    cost = token_usage['total_tokens'] * price\n",
    "    cost = round(cost, 3)\n",
    "    print(f'Token usage: {token_usage}')\n",
    "    print(f'Cost: ${cost} or ${cost * 1000} per 1000 Questions')\n",
    "    result[\"cost\"] = cost\n",
    "    return result\n",
    "\n",
    "\n",
    "openai_results = [ask_openai(api3_prompt), ask_openai(wood_prompt)]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "| Cost | Performance | Quality |\n",
    "|------|-------------|---------|\n",
    "| 1  |     2      |    5   |\n",
    "\n",
    "At $0.03 per question, this will probably be our most expensive model. Average response time was around 5 seconds. I know from experience they have a lot of downtime. The answer quality is amazing. Exactly what I am looking for. Truly SOTA.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohere AI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have tried this one. Its pretty good and cheaper than OpenAI but not by much. Gonna sign up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Answer:\n",
       "\n",
       "----------------\n",
       "\n",
       "\n",
       " You can use the `deriveAirnodeXpub` function in the [@airnode/airnode-admin](https://docs.api3.org/airnode/v0.10/) package in Node.js like this:\n",
       "\n",
       "```js\n",
       "const xpub = await deriveAirnodeXpub(airnodeXpub, airnodeAddress);\n",
       "```\n",
       "\n",
       "This will derive the xpub from the Airnode xpub and the Airnode address.\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "----------------\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: $0.011 or $11.0 per 1000 Questions\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Answer:\n",
       "\n",
       "----------------\n",
       "\n",
       "\n",
       " Wait until the caulk dries and then apply the mold release.\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "----------------\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: $0.011 or $11.0 per 1000 Questions\n"
     ]
    }
   ],
   "source": [
    "cohere = Cohere(cohere_api_key=keys['cohere'], temperature=0.5)\n",
    "# 2.5 USD per 1000 units of 1000 characters. Price per character\n",
    "price = (2.5 / 1000) / 1000\n",
    "\n",
    "\n",
    "def ask_cohere(prompt):\n",
    "    result = ask(cohere, prompt)\n",
    "    render_answer(result['answer'])\n",
    "    print(f'Duration: {result[\"duration\"]} seconds')\n",
    "    total_characters = len(result['answer']) + len(prompt)\n",
    "    cost = total_characters * price\n",
    "    cost = round(cost, 3)\n",
    "    print(f'Cost: ${cost} or ${cost * 1000} per 1000 Questions')\n",
    "    result['cost'] = cost\n",
    "    return result\n",
    "\n",
    "\n",
    "cohere_results = [ask_cohere(api3_prompt), ask_cohere(wood_prompt)]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "| Cost | Performance | Quality |\n",
    "|------|-------------|---------|\n",
    "| 4  |     2      |    1   |\n",
    "\n",
    "\n",
    "Much better price! Only around ~$0.011 per question! Completely fails both questions. Much slower too. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI21"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Never heard of this one. Lets try it out.\n",
    "\n",
    "Simple sign up. Pretty cool, flat fee for each request. Only paying for response tokens! I like that. Could be the solution here since we have huge inputs but tiny responses.\n",
    "\n",
    "Price: $0.25 per 1000 tokens + $0.005 per request\n",
    "\n",
    "Signed up and got my API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Answer:\n",
       "\n",
       "----------------\n",
       "\n",
       "\n",
       " You can use the `deriveAirnodeXpub` function in the [@airnode/airnode-admin](https://docs.api3.org/airnode/v0.10/) package in Node.js like this:\n",
       "\n",
       "```js\n",
       "const airnodeXpub = await deriveAirnodeXpub(airnodeMnemonic);\n",
       "```\n",
       "\n",
       "This will derive the Airnode xpub from the Airnode mnemonic.\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "----------------\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 5.258299112319946 seconds\n",
      "Tokens: 97\n",
      "Cost: $0.029 or $29.0 per 1000 Questions\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Answer:\n",
       "\n",
       "----------------\n",
       "\n",
       "\n",
       " Wait until the caulk dries.\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "----------------\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 1.3200790882110596 seconds\n",
      "Tokens: 9\n",
      "Cost: $0.007 or $7.0 per 1000 Questions\n"
     ]
    }
   ],
   "source": [
    "ai21 = AI21(ai21_api_key=keys['ai21'], temperature=0.5, maxTokens=1000)\n",
    "\n",
    "price = 0.25 / 1000  # price per token\n",
    "\n",
    "def ask_ai21(prompt):\n",
    "    result = ask(ai21, prompt)\n",
    "    render_answer(result['answer'])\n",
    "    print(f'Duration: {result[\"duration\"]} seconds')\n",
    "    tokens = AI21.get_num_tokens(AI21, result['answer'])\n",
    "    print(f'Tokens: {tokens}')\n",
    "    cost = tokens * price\n",
    "    cost = round(cost, 3)\n",
    "    cost += 0.005\n",
    "    print(f'Cost: ${cost} or ${cost * 1000} per 1000 Questions')\n",
    "    result['cost'] = cost\n",
    "    return result\n",
    "\n",
    "ai21_results = [ask_ai21(api3_prompt), ask_ai21(wood_prompt)]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "| Cost | Performance | Quality |\n",
    "|------|-------------|---------|\n",
    "| 3  |     5      |    5   |\n",
    "\n",
    "\n",
    "This was amazing! The price is great and the response time was great. The quality was also great. I think this is the winner.\n",
    "\n",
    "The longer responses are going to end up costing a lot more. Good news is the J1-Grande-beta is way cheaper and seems to be working almost nearly as well. Would definitely work well for non-code!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb2c640052a679058fa963edb19c73c9244639affc4049c356a2be62de116d7b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
