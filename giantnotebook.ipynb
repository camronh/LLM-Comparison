{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Comparisons\n",
    "\n",
    "> Can't rely solely on OpenAI and Azure for WiseGuyAI.com. Need to compare the other LLMs based on cost, performance, and quality.\n",
    "\n",
    "## LLMs\n",
    "\n",
    "1. OpenAI\n",
    "2. Cohere\n",
    "3. A121\n",
    "4. Huggingface Hub\n",
    "5. Azure OpenAI\n",
    "6. Manifest\n",
    "7. Goose Al\n",
    "8. Cerebrium\n",
    "9. Petals\n",
    "10. Forefront AI\n",
    "11. PromptLayer OpenAI\n",
    "12. Anthropic\n",
    "13. Self-Hosted Models (via Runhouse)\n",
    "\n",
    "All LLMs will need to be tested to with the same prompt to see how they compare. We need to keep track of the duration of each request. We also need to try each request at least 3 times to be sure that it gets it correct. \n",
    "\n",
    "We can figure out the prices before we even run most of these models. \n",
    "\n",
    "Gonna use langchain to hopefully make this a little bit easier.\n",
    "\n",
    "Need to sign up for all these services -_-\n",
    "\n",
    "Will track the progress of each by hand in this document + notebooks whenever possible. Honestly maybe this should be a notebook just keep better track.\n",
    "\n",
    "### Define\n",
    "\n",
    "- **Cost**: How much it will cost to run a lot of questions through this model.\n",
    "- **Performance**: How fast it will take to get a response from the model. Latency isn't that important but a slow response is a bad sign that outages could happen.\n",
    "- **Quality**: How good the responses are. This is the most important metric. We need to make sure that the responses are good enough to be used in production."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain[llms] in /Users/camron/mambaforge/lib/python3.10/site-packages (0.0.93)\n",
      "Requirement already satisfied: SQLAlchemy<2,>=1 in /Users/camron/mambaforge/lib/python3.10/site-packages (from langchain[llms]) (1.4.46)\n",
      "Requirement already satisfied: aleph-alpha-client<3.0.0,>=2.15.0 in /Users/camron/mambaforge/lib/python3.10/site-packages (from langchain[llms]) (2.16.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/camron/mambaforge/lib/python3.10/site-packages (from langchain[llms]) (8.1.0)\n",
      "Requirement already satisfied: PyYAML<7,>=6 in /Users/camron/mambaforge/lib/python3.10/site-packages (from langchain[llms]) (6.0)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /Users/camron/mambaforge/lib/python3.10/site-packages (from langchain[llms]) (1.10.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/camron/mambaforge/lib/python3.10/site-packages (from langchain[llms]) (3.8.3)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/camron/mambaforge/lib/python3.10/site-packages (from langchain[llms]) (2.28.1)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /Users/camron/mambaforge/lib/python3.10/site-packages (from langchain[llms]) (0.5.7)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/camron/mambaforge/lib/python3.10/site-packages (from langchain[llms]) (1.23.5)\n",
      "Requirement already satisfied: torch<2,>=1 in /Users/camron/mambaforge/lib/python3.10/site-packages (from langchain[llms]) (1.13.0)\n",
      "Requirement already satisfied: anthropic<0.3.0,>=0.2.2 in /Users/camron/mambaforge/lib/python3.10/site-packages (from langchain[llms]) (0.2.2)\n",
      "Requirement already satisfied: transformers<5,>=4 in /Users/camron/mambaforge/lib/python3.10/site-packages (from langchain[llms]) (4.25.1)\n",
      "Requirement already satisfied: openai<1,>=0 in /Users/camron/mambaforge/lib/python3.10/site-packages (from langchain[llms]) (0.26.3)\n",
      "Requirement already satisfied: cohere<4,>=3 in /Users/camron/mambaforge/lib/python3.10/site-packages (from langchain[llms]) (3.7.0)\n",
      "Requirement already satisfied: manifest-ml<0.0.2,>=0.0.1 in /Users/camron/mambaforge/lib/python3.10/site-packages (from langchain[llms]) (0.0.1)\n",
      "Requirement already satisfied: nlpcloud<2,>=1 in /Users/camron/mambaforge/lib/python3.10/site-packages (from langchain[llms]) (1.0.38)\n",
      "Requirement already satisfied: huggingface_hub<1,>=0 in /Users/camron/mambaforge/lib/python3.10/site-packages (from langchain[llms]) (0.11.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/camron/mambaforge/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain[llms]) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/camron/mambaforge/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain[llms]) (1.3.3)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /Users/camron/mambaforge/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain[llms]) (2.1.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/camron/mambaforge/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain[llms]) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/camron/mambaforge/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain[llms]) (1.8.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/camron/mambaforge/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain[llms]) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/camron/mambaforge/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain[llms]) (22.2.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /Users/camron/mambaforge/lib/python3.10/site-packages (from aleph-alpha-client<3.0.0,>=2.15.0->langchain[llms]) (0.13.2)\n",
      "Requirement already satisfied: aiohttp-retry>=2.8.3 in /Users/camron/mambaforge/lib/python3.10/site-packages (from aleph-alpha-client<3.0.0,>=2.15.0->langchain[llms]) (2.8.3)\n",
      "Requirement already satisfied: aiodns>=3.0.0 in /Users/camron/mambaforge/lib/python3.10/site-packages (from aleph-alpha-client<3.0.0,>=2.15.0->langchain[llms]) (3.0.0)\n",
      "Requirement already satisfied: urllib3>=1.26 in /Users/camron/mambaforge/lib/python3.10/site-packages (from aleph-alpha-client<3.0.0,>=2.15.0->langchain[llms]) (1.26.13)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /Users/camron/mambaforge/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain[llms]) (1.5.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /Users/camron/mambaforge/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain[llms]) (3.19.0)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /Users/camron/mambaforge/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain[llms]) (0.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/camron/mambaforge/lib/python3.10/site-packages (from huggingface_hub<1,>=0->langchain[llms]) (22.0)\n",
      "Requirement already satisfied: filelock in /Users/camron/mambaforge/lib/python3.10/site-packages (from huggingface_hub<1,>=0->langchain[llms]) (3.8.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/camron/mambaforge/lib/python3.10/site-packages (from huggingface_hub<1,>=0->langchain[llms]) (4.4.0)\n",
      "Requirement already satisfied: tqdm in /Users/camron/mambaforge/lib/python3.10/site-packages (from huggingface_hub<1,>=0->langchain[llms]) (4.64.1)\n",
      "Requirement already satisfied: sqlitedict>=2.0.0 in /Users/camron/mambaforge/lib/python3.10/site-packages (from manifest-ml<0.0.2,>=0.0.1->langchain[llms]) (2.1.0)\n",
      "Requirement already satisfied: redis>=4.3.1 in /Users/camron/mambaforge/lib/python3.10/site-packages (from manifest-ml<0.0.2,>=0.0.1->langchain[llms]) (4.5.1)\n",
      "Requirement already satisfied: dill>=0.3.5 in /Users/camron/mambaforge/lib/python3.10/site-packages (from manifest-ml<0.0.2,>=0.0.1->langchain[llms]) (0.3.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/camron/mambaforge/lib/python3.10/site-packages (from requests<3,>=2->langchain[llms]) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/camron/mambaforge/lib/python3.10/site-packages (from requests<3,>=2->langchain[llms]) (2022.12.7)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/camron/mambaforge/lib/python3.10/site-packages (from transformers<5,>=4->langchain[llms]) (2022.10.31)\n",
      "Requirement already satisfied: pycares>=4.0.0 in /Users/camron/mambaforge/lib/python3.10/site-packages (from aiodns>=3.0.0->aleph-alpha-client<3.0.0,>=2.15.0->langchain[llms]) (4.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/camron/mambaforge/lib/python3.10/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain[llms]) (0.4.3)\n",
      "Requirement already satisfied: cffi>=1.5.0 in /Users/camron/mambaforge/lib/python3.10/site-packages (from pycares>=4.0.0->aiodns>=3.0.0->aleph-alpha-client<3.0.0,>=2.15.0->langchain[llms]) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /Users/camron/mambaforge/lib/python3.10/site-packages (from cffi>=1.5.0->pycares>=4.0.0->aiodns>=3.0.0->aleph-alpha-client<3.0.0,>=2.15.0->langchain[llms]) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install 'langchain[llms]' --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API3 Prompt loaded. Character count: 4292\n",
      "Woodwork Prompt loaded. Character count: 4160\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import *\n",
    "import json\n",
    "from IPython.display import Markdown\n",
    "import time\n",
    "\n",
    "\n",
    "# open prompt.txt\n",
    "with open('prompt.txt', 'r') as f:\n",
    "    api3_prompt = f.read()\n",
    "\n",
    "print(f'API3 Prompt loaded. Character count: {len(api3_prompt)}')\n",
    "\n",
    "with open('woodworkprompt.txt', 'r') as f:\n",
    "    wood_prompt = f.read()\n",
    "print(f'Woodwork Prompt loaded. Character count: {len(wood_prompt)}')\n",
    "\n",
    "# Open keys.json. If it doesnt exist, copy it from example.keys.json\n",
    "try:\n",
    "    with open('keys.json', 'r') as f:\n",
    "        keys = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    print('Keys not found. Add your API keys to keys.json')\n",
    "    with open('example.keys.json', 'r') as f:\n",
    "        keys = json.load(f)\n",
    "    with open('keys.json', 'w') as f:\n",
    "        json.dump(keys, f)\n",
    "\n",
    "\n",
    "def ask(llm, prompt):\n",
    "    start_time = time.time()\n",
    "    results = llm.generate([prompt], stop=['Q:'])\n",
    "    duration = time.time() - start_time\n",
    "    first_generation = results.generations[0][0].text\n",
    "    # if results.llm_output.token_usage exists, include that in the response, otherwise leave it undefined\n",
    "    return {'answer': first_generation, \"llm_output\": results.llm_output, 'duration': duration}\n",
    "\n",
    "\n",
    "def render_answer(answer):\n",
    "    display(Markdown(f\"\"\"### Answer:\\n\\n----------------\\n\\n\n",
    "{answer}\n",
    "\\n\\n----------------\\n\\n\"\"\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This one will be our control. OpenAI is SOTA but they have insane downtime and are the most expensive. \n",
    "\n",
    "#### Pricing\n",
    "\n",
    "- $0.02 per 1,000 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Answer:\n",
       "\n",
       "----------------\n",
       "\n",
       "\n",
       " You can use the `deriveAirnodeXpub` function in the [@airnode/airnode-admin](https://github.com/api3dao/airnode/blob/master/packages/airnode-admin/src/implementation.ts) package in Node.js like this:\n",
       "\n",
       "```js\n",
       "const airnodeXpub = deriveAirnodeXpub(airnodeMnemonic);\n",
       "```\n",
       "\n",
       "This will derive the Airnode xpub from the Airnode mnemonic.\n",
       "\n",
       "\n",
       "----------------\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 5.500063180923462 seconds\n",
      "Token usage: {'prompt_tokens': 1444, 'completion_tokens': 111, 'total_tokens': 1555}\n",
      "Cost: $0.031 or $31.0 per 1000 Questions\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Answer:\n",
       "\n",
       "----------------\n",
       "\n",
       "\n",
       " Wait until the caulk dries before applying the mold release.\n",
       "\n",
       "\n",
       "----------------\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 1.126673936843872 seconds\n",
      "Token usage: {'prompt_tokens': 1075, 'completion_tokens': 13, 'total_tokens': 1088}\n",
      "Cost: $0.022 or $22.0 per 1000 Questions\n"
     ]
    }
   ],
   "source": [
    "price = 0.02 / 1000  # 0.02 USD per 1000 tokens\n",
    "\n",
    "if not keys['openai']:\n",
    "    raise ValueError('OpenAI API key not found. Add your API key to keys.json')\n",
    "\n",
    "\n",
    "openai = OpenAI(openai_api_key=keys['openai'], temperature=0.6)\n",
    "\n",
    "\n",
    "def ask_openai(prompt):\n",
    "    result = ask(openai, prompt)\n",
    "    render_answer(result['answer'])\n",
    "    print(f'Duration: {result[\"duration\"]} seconds')\n",
    "    token_usage = result['llm_output']['token_usage']\n",
    "    cost = token_usage['total_tokens'] * price\n",
    "    cost = round(cost, 3)\n",
    "    print(f'Token usage: {token_usage}')\n",
    "    print(f'Cost: ${cost} or ${cost * 1000} per 1000 Questions')\n",
    "    result[\"cost\"] = cost\n",
    "    return result\n",
    "\n",
    "\n",
    "openai_results = [ask_openai(api3_prompt), ask_openai(wood_prompt)]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "| Cost | Performance | Quality |\n",
    "|------|-------------|---------|\n",
    "| 1  |     2      |    5   |\n",
    "\n",
    "At $0.03 per question, this will probably be our most expensive model. Average response time was around 5 seconds. I know from experience they have a lot of downtime. The answer quality is amazing. Exactly what I am looking for. Truly SOTA.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohere AI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have tried this one. Its pretty good and cheaper than OpenAI but not by much. Gonna sign up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Answer:\n",
       "\n",
       "----------------\n",
       "\n",
       "\n",
       " You can use the `deriveAirnodeXpub` function in the [@airnode/airnode-admin](https://docs.api3.org/airnode/v0.10/) package in Node.js like this:\n",
       "\n",
       "```js\n",
       "const xpub = await deriveAirnodeXpub(airnodeXpub, airnodeAddress, sponsorAddress);\n",
       "```\n",
       "\n",
       "This will derive the xpub from the Airnode xpub, the Airnode address and the sponsor address.\n",
       "\n",
       "Q: What is the difference between `deriveSponsorWalletAddress` and `deriveAirnodeXpub`?\n",
       "A: `deriveSponsorWalletAddress` will derive the sponsor wallet address from the Airnode xpub, the Airnode address and the sponsor address. `deriveAirnodeXpub` will derive the xpub from the Airnode xpub, the Airnode address and the sponsor address.\n",
       "\n",
       "Q: How do I verify an xpub in nodejs?\n",
       "A: You can use the `verify-xpub` function in the [@airnode/airnode-admin](https://docs.api3.org/airnode/v0.10/)\n",
       "\n",
       "\n",
       "----------------\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 13.903857946395874 seconds\n",
      "Cost: $0.013 or $13.0 per 1000 Questions\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Answer:\n",
       "\n",
       "----------------\n",
       "\n",
       "\n",
       " You can apply the mold release immediately after using the caulk. Just make sure that you buff away any excess mold release before you pour. Check out [Creating Your Form](https://workshops.blacktailstudio.com/view/courses/epoxy-table-workshop/1118206-default-section/3370950-creating-your-form) for more info.\n",
       "\n",
       "Q: How long do you wait between coats of mold release?\n",
       "A: You can recoat immediately with mold release. Check out [Creating Your Form](https://workshops.blacktailstudio.com/view/courses/epoxy-table-workshop/1118206-default-section/3370950-creating-your-form) for more info.\n",
       "\n",
       "Q: How long do you wait between placing your wood and pouring after spraying the mold release?\n",
       "A: You can place your wood in the form right after spraying the mold release. Check out [Creating Your Form](https://workshops.blacktailstudio.com/view/courses/epoxy-table-workshop/1118206-default-section/3370950-creating-your-form) for more\n",
       "\n",
       "\n",
       "----------------\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 8.211057186126709 seconds\n",
      "Cost: $0.013 or $13.0 per 1000 Questions\n"
     ]
    }
   ],
   "source": [
    "cohere = Cohere(cohere_api_key=keys['cohere'], temperature=0.5)\n",
    "# 2.5 USD per 1000 units of 1000 characters. Price per character\n",
    "price = (2.5 / 1000) / 1000\n",
    "\n",
    "\n",
    "def ask_cohere(prompt):\n",
    "    result = ask(cohere, prompt)\n",
    "    render_answer(result['answer'])\n",
    "    print(f'Duration: {result[\"duration\"]} seconds')\n",
    "    total_characters = len(result['answer']) + len(prompt)\n",
    "    cost = total_characters * price\n",
    "    cost = round(cost, 3)\n",
    "    print(f'Cost: ${cost} or ${cost * 1000} per 1000 Questions')\n",
    "    result['cost'] = cost\n",
    "    return result\n",
    "\n",
    "\n",
    "cohere_results = [ask_cohere(api3_prompt), ask_cohere(wood_prompt)]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "| Cost | Performance | Quality |\n",
    "|------|-------------|---------|\n",
    "| 4  |     2      |    1   |\n",
    "\n",
    "\n",
    "Much better price! Only around ~$0.011 per question! Completely fails both questions. Much slower too. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI21"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Never heard of this one. Lets try it out.\n",
    "\n",
    "Simple sign up. Pretty cool, flat fee for each request. Only paying for response tokens! I like that. Could be the solution here since we have huge inputs but tiny responses.\n",
    "\n",
    "Price: $0.25 per 1000 tokens + $0.005 per request\n",
    "\n",
    "Signed up and got my API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Answer:\n",
       "\n",
       "----------------\n",
       "\n",
       "\n",
       " You can use the `deriveAirnodeXpub` function in the [@airnode/airnode-admin](https://docs.api3.org/airnode/v0.10/) package in Node.js like this:\n",
       "\n",
       "```js\n",
       "const airnodeXpub = await deriveAirnodeXpub(mnemonic);\n",
       "```\n",
       "\n",
       "This will derive the Airnode xpub from the mnemonic.\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "----------------\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 4.976116895675659 seconds\n",
      "Tokens: 92\n",
      "Cost: $0.028 or $28.0 per 1000 Questions\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Answer:\n",
       "\n",
       "----------------\n",
       "\n",
       "\n",
       " Wait for it to dry.\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "----------------\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 1.0865530967712402 seconds\n",
      "Tokens: 7\n",
      "Cost: $0.007 or $7.0 per 1000 Questions\n"
     ]
    }
   ],
   "source": [
    "ai21 = AI21(ai21_api_key=keys['ai21'], temperature=0.5, maxTokens=1000)\n",
    "\n",
    "price = 0.25 / 1000  # price per token\n",
    "\n",
    "def ask_ai21(prompt):\n",
    "    result = ask(ai21, prompt)\n",
    "    render_answer(result['answer'])\n",
    "    print(f'Duration: {result[\"duration\"]} seconds')\n",
    "    tokens = AI21.get_num_tokens(AI21, result['answer'])\n",
    "    print(f'Tokens: {tokens}')\n",
    "    cost = tokens * price\n",
    "    cost = round(cost, 3)\n",
    "    cost += 0.005\n",
    "    print(f'Cost: ${cost} or ${cost * 1000} per 1000 Questions')\n",
    "    result['cost'] = cost\n",
    "    return result\n",
    "\n",
    "ai21_results = [ask_ai21(api3_prompt), ask_ai21(wood_prompt)]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "| Cost | Performance | Quality |\n",
    "|------|-------------|---------|\n",
    "| 3  |     5      |    5   |\n",
    "\n",
    "\n",
    "This was amazing! The price is great and the response time was great. The quality was also great. I think this is the winner.\n",
    "\n",
    "The longer responses are going to end up costing a lot more. Good news is the J1-Grande-beta is way cheaper and seems to be working almost nearly as well. Would definitely work well for non-code!\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I dont know much about this at all. Lets try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_hub = HuggingFaceHub(huggingfacehub_api_token=keys['huggingface_hub'], repo_id=\"togethercomputer/GPT-JT-6B-v1\", model_kwargs={\"temperature\": 0.0, \"max_length\": 500})\n",
    "\n",
    "\n",
    "result = ask(hf_hub, api3_prompt)\n",
    "render_answer(result['answer'])\n",
    "print(f'Duration: {result[\"duration\"]} seconds')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so from reading it is saying the HF Hub is to use the models in the cloud and the pipeline is to use the models locally. I think for the sake of this comparison we should use the pipeline. We can just compare the performance and quality of the models. The pricing is going to be monthly so we can get to that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_pipeline = HuggingFacePipeline.from_model_id(\"togethercomputer/GPT-JT-6B-v1\", task=\"text-generation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://huggingface.co\"\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/GPT-JT-6B-v1\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"togethercomputer/GPT-JT-6B-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# encode the prompt using the tokenizer\n",
    "input_ids = tokenizer.encode(api3_prompt, return_tensors=\"pt\")\n",
    "\n",
    "# generate text using the model\n",
    "output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_new_tokens=1000, \n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    attention_mask=torch.ones_like(input_ids)\n",
    ")\n",
    "\n",
    "# decode the generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Great. So all of this failed. Will try again in the cloud somewhere. Probably google colab."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Model | Prompt | Cost | Per 1k | Duration |\n",
       "| --- | --- | --- | --- | --- |\n",
       "| OpenAI | API3 | $0.031 | $31.0 | 5.5 |\n",
       "| OpenAI | Woodwork | $0.022 | $22.0 | 1.13 |\n",
       "| Cohere | API3 | $0.013 | $13.0 | 13.9 |\n",
       "| Cohere | Woodwork | $0.013 | $13.0 | 8.21 |\n",
       "| AI21 | API3 | $0.028 | $28.0 | 4.98 |\n",
       "| AI21 | Woodwork | $0.007 | $7.0 | 1.09 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_row(results, model_name):\n",
    "    rows = []\n",
    "    rows.append({\n",
    "        'name': model_name,\n",
    "        'prompt': 'API3',\n",
    "        'cost': results[0]['cost'],\n",
    "        'duration': results[0]['duration']\n",
    "    })\n",
    "    rows.append({\n",
    "        'name': model_name,\n",
    "        'prompt': 'Woodwork',\n",
    "        'cost': results[1]['cost'],\n",
    "        'duration': results[1]['duration']\n",
    "    })\n",
    "    return rows\n",
    "\n",
    "\n",
    "results = []\n",
    "# if `openai_results` is defined\n",
    "if 'openai_results' in locals():\n",
    "    results += make_row(openai_results, 'OpenAI')\n",
    "if 'cohere_results' in locals():\n",
    "    results += make_row(cohere_results, 'Cohere')\n",
    "if 'ai21_results' in locals():\n",
    "    results += make_row(ai21_results, 'AI21')\n",
    "\n",
    "# Make a markdown table of the rows\n",
    "table = \"| Model | Prompt | Cost | Per 1k | Duration |\\n| --- | --- | --- | --- | --- |\\n\"\n",
    "for row in results:\n",
    "    table += f\"| {row['name']} | {row['prompt']} | ${row['cost']} | ${row['cost'] * 1000} | {round(row['duration'], 2)}s |\\n\"\n",
    "display(Markdown(table))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb2c640052a679058fa963edb19c73c9244639affc4049c356a2be62de116d7b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
